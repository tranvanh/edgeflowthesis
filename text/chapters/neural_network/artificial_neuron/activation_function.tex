An artificial neuron's activation function defines neuron's output value for given inputs, commonly being ${f: \mathbb{R} \rightarrow \mathbb{R}}$ \cite{leskovec2020mining}. An important trait of many activation functions is their differentiability, allowing them to be used for \textit{Backpropagation}, ANN training algorithm. The activation function needs to have a derivative that does not saturate when headed towards 0 or explode when headed towards inf \cite{matous}.

For such reasons, step function or any linear function are unsuitable for ANN.
% Sigmoid Function ==========================================================================================================
\setsecnumdepth{all}
\subsubsection{Sigmoid Function}
The sigmoid function is often used in ANN as an alternative to the step function. A popular choice of the sigmoid function is a \textit{logistic sigmoid}, output value ranging between 0 and 1.

\begin{equation}
    {\sigma(\alpha) = \frac{1}{1 + e^{-\alpha}} = \frac{e^x}{1 + e^{x}}}
\end{equation}


% \begin{figure}[h]
%   \centering
%     \includegraphics[width=7cm]{sigmoid}
%   \caption{Sigmoid function}
%   \label{fig:sigmoid}
% \end{figure}


One of the reasons for its popularity is the simplicity of its derivative calculation:

\begin{equation}
    {\frac{d}{dx}\sigma(\alpha) = \frac{e^x}{(1 + e^{x})^2} = \sigma(x)(1-\sigma(x))}
\end{equation}


Its disadvantages is the \textit{vanishing gradient}. A problem where if given a very high or very low input values, the prediction would stay almost the same. Possibly resulting in training complications or performance issues \cite{7typesactivationfunctions}, \cite{matous}.

% Hyperbolic Tangent ==========================================================================================================

\subsubsection{Hyperbolic Tangent}

Hyperbolic tangent is similar to logistic sigmoid function with a key difference in its output, ranging between -1 and 1.

\begin{equation}
    {tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}}
\end{equation}


\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{tangent}
    \caption{Hyperbolic tangent \cite{matous}}
    \label{fig:hyperbolictangent}
\end{figure}


It shares the sigmoid's simple calculation of its derivative.

\begin{equation}
    {\frac{d}{dx}\tanh(x) = 1 - \frac{(e^x - e^{-x})^2}{(e^x + e^{-x})^2} = 1 -\tanh^2(x)}
\end{equation}

By being only moved and scaled version of the sigmoid function, hyperbolic tangent shares not only sigmoid's advantages but also its disadvantages \cite{leskovec2020mining}, \cite{matous}.

% Rectified Linear Unit ==========================================================================================================

\subsubsection{Rectified Linear Unit}

The output of the Rectified Linear Unit (ReLU) is defined as:

\begin{equation}
    f(x) = max(0,x)
\begin{cases}
    x, & \text{if $x\ \geq\ 0$}\\
    0, & \text{if $x\ <\ 0$}
\end{cases} 
\end{equation} 

\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{relu}
    \caption{Rectified Linear Unit \cite{matous}}
    \label{fig:relu}
\end{figure}


ReLU's popularity is mainly due to its computational efficiency \cite{7typesactivationfunctions}. Its disadvantages begin to show themselves once inputs approach zero or to a negative number. Causing the so-called dying ReLu issue, where the network is unable to learn anymore. There are many variations of ReLu to this date, e.g., Leaky ReLU, Parametric ReLU, ELU, ...
% Softmax ==========================================================================================================

\subsubsection{Softmax}

Softmax separates itself from all the previously mentioned functions by its ability to handle multiple input values in the form of a vector $\vec{x} = (x_1,x_2,...,x_n)$ and output for each $x_i$ defined as:

\begin{equation}
    {\sigma(x_i) = \frac{e^x_i}{\sum_{j=1}^{n}e^x_j}}
\end{equation}

Output is normalized probability distribution, ensuring $\sum_{i}\sigma(x_i) = 1$ \cite{lipton2015critical}. It is being used as the last activation function of ANN, normalizing the network's output into $n$ probability groups.
